{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6RxqIirisTj"
      },
      "source": [
        "# HW3\n",
        "\n",
        "In this homework, we'll learn about transformers and chatbots.\n",
        "\n",
        "It will probably be easiest to run this on http://colab.research.google.com"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## minGPT Character Language Model\n",
        "\n",
        "First, will inspect Karpathy's [minGPT](https://github.com/karpathy/minGPT/tree/master) library to learn more about transformers.\n",
        "\n",
        "We'll first fit a character language model using mingpt. We'll use as training data all the text of Shakespeare."
      ],
      "metadata": {
        "id": "whaHOZpKj_-X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# clone the library\n",
        "!git clone https://github.com/karpathy/minGPT.git"
      ],
      "metadata": {
        "id": "f_q_lNGEjs7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add mingpt to your Python path, so you can import it.\n",
        "import sys\n",
        "sys.path.insert(0, './minGPT')\n",
        "from mingpt.model import GPT\n",
        "from mingpt.trainer import Trainer\n",
        "from mingpt.utils import set_seed\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "set_seed(3407)"
      ],
      "metadata": {
        "id": "eDv1T3BNjwzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# download shakespeare data\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ],
      "metadata": {
        "id": "mPHv0dGquVpn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data loading and training code"
      ],
      "metadata": {
        "id": "fOnS25h6iSAB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from mingpt.utils import set_seed, setup_logging, CfgNode as CN\n",
        "import os\n",
        "import sys\n",
        "\n",
        "class CharDataset(Dataset):\n",
        "    \"\"\"\n",
        "    This represents a dataset of characters.\n",
        "    \"\"\"\n",
        "    @staticmethod\n",
        "    def get_default_config():\n",
        "        C = CN()\n",
        "        C.block_size = 128\n",
        "        return C\n",
        "\n",
        "    def __init__(self, config, data):\n",
        "        self.config = config\n",
        "        self.parse_data(data)\n",
        "\n",
        "    def parse_data(self, data):\n",
        "        print('parsing char data')\n",
        "        # get list of all characters\n",
        "        chars = sorted(list(set(data)))\n",
        "        data_size, vocab_size = len(data), len(chars)\n",
        "        print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
        "        # map from char to int\n",
        "        self.stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "        # map from into to char\n",
        "        self.itos = { i:ch for i,ch in enumerate(chars) }\n",
        "        self.vocab_size = vocab_size\n",
        "        self.data = data\n",
        "\n",
        "    def get_vocab_size(self):\n",
        "        return self.vocab_size\n",
        "\n",
        "    def get_block_size(self):\n",
        "        return self.config.block_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.config.block_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # grab a chunk of (block_size + 1) characters from the data\n",
        "        chunk = self.data[idx:idx + self.config.block_size + 1]\n",
        "        # encode every character to an integer\n",
        "        dix = [self.stoi[s] for s in chunk]\n",
        "        # return as tensors\n",
        "        x = torch.tensor(dix[:-1], dtype=torch.long)\n",
        "        y = torch.tensor(dix[1:], dtype=torch.long)\n",
        "        return x, y\n",
        "\n",
        "def get_config():\n",
        "\n",
        "    C = CN()\n",
        "\n",
        "    # system\n",
        "    C.system = CN()\n",
        "    C.system.seed = 3407\n",
        "    C.system.work_dir = './out'\n",
        "\n",
        "    # data\n",
        "    C.data = CharDataset.get_default_config()\n",
        "\n",
        "    # model\n",
        "    C.model = GPT.get_default_config()\n",
        "    C.model.model_type = 'gpt-micro'\n",
        "\n",
        "    # trainer\n",
        "    C.trainer = Trainer.get_default_config()\n",
        "    C.trainer.learning_rate = 5e-4 # the model we're using is so small that we can go a bit faster\n",
        "\n",
        "    return C\n",
        "\n",
        "\n",
        "def train_model(config, train_dataset, sample_fn):\n",
        "    \"\"\"\n",
        "    Train the model.\n",
        "    config..........CfgNode\n",
        "    train_dataset...Dataset that emits strings for training\n",
        "    sample_fn.......function to call during training to show sample output.\n",
        "    \"\"\"\n",
        "    # construct the model\n",
        "    config.model.vocab_size = train_dataset.get_vocab_size()\n",
        "    config.model.block_size = train_dataset.get_block_size()\n",
        "    model = GPT(config.model)\n",
        "\n",
        "    # construct the trainer object\n",
        "    trainer = Trainer(config.trainer, model, train_dataset)\n",
        "\n",
        "    # iteration callback\n",
        "    def batch_end_callback(trainer):\n",
        "\n",
        "        if trainer.iter_num % 10 == 0:\n",
        "            print(f\"iter_dt {trainer.iter_dt * 1000:.2f}ms; iter {trainer.iter_num}: train loss {trainer.loss.item():.5f}\")\n",
        "\n",
        "        if trainer.iter_num % 500 == 0:\n",
        "            # evaluate both the train and test score\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                # sample from the model...\n",
        "                context = list(train_dataset.itos.values())[0]\n",
        "                completion = sample_fn(context, model, trainer, train_dataset, maxlen=100, temperature=1.)\n",
        "                print('sample from the model:')\n",
        "                print(completion)\n",
        "            # save the latest model\n",
        "            print(\"saving model\")\n",
        "            ckpt_path = os.path.join(config.system.work_dir, \"model.pt\")\n",
        "            torch.save(model.state_dict(), ckpt_path)\n",
        "            # revert model to training mode\n",
        "            model.train()\n",
        "\n",
        "    trainer.set_callback('on_batch_end', batch_end_callback)\n",
        "\n",
        "    # run the optimization\n",
        "    trainer.run()\n",
        "    model.eval()\n",
        "    return model, trainer\n",
        "\n",
        "def configure_model(max_iters=100, block_size=128):\n",
        "    config = get_config()\n",
        "    config.merge_from_args(['--trainer.max_iters=%d' % max_iters,\n",
        "                            '--data.block_size=%d' % block_size,\n",
        "                            '--model.block_size=%d' % block_size])\n",
        "    setup_logging(config)\n",
        "    set_seed(config.system.seed)\n",
        "    return config\n",
        "\n",
        "\n",
        "def create_char_data(config):\n",
        "    # construct the training dataset\n",
        "    text = open('input.txt', 'r').read()\n",
        "    return CharDataset(config.data, text)\n",
        "\n",
        "def sample_from_char_model(context, model, trainer, train_dataset, maxlen=500, temperature=1.):\n",
        "    x = torch.tensor([train_dataset.stoi[s] for s in context], dtype=torch.long)[None,...].to(trainer.device)\n",
        "    y = model.generate(x, maxlen, temperature=temperature, do_sample=True, top_k=10)[0]\n",
        "    return ''.join([train_dataset.itos[int(i)] for i in y])"
      ],
      "metadata": {
        "id": "frsP7S3DuMhT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train the character model.\n",
        "config = configure_model(max_iters=100, block_size=64)\n",
        "train_dataset = create_char_data(config)\n",
        "model, trainer = train_model(config, train_dataset, sample_from_char_model)"
      ],
      "metadata": {
        "id": "hpp2SypliPPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sample_from_char_model(\"Romeo:\", model, trainer, train_dataset, maxlen=10, temperature=1))"
      ],
      "metadata": {
        "id": "ty_WfOozuVN9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is the `block_size` variable? Describe in detail what it does.**\n",
        "\n",
        "You might want to consult the code for [model.py](https://github.com/karpathy/minGPT/blob/master/mingpt/model.py).\n",
        "\n"
      ],
      "metadata": {
        "id": "jB30f-tj-uyl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**YOUR ANSWER**"
      ],
      "metadata": {
        "id": "jgMZ82gwi1ct"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is the relationship between `block_size` and the total number of parameters in the model?** That is, if we double `block_size`, what happens to the total number of model parameters?"
      ],
      "metadata": {
        "id": "oppLFY7m_yiy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**YOUR ANSWER**"
      ],
      "metadata": {
        "id": "zMMlhs6ji9eW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is the `n_layer` parameter? Describe in detail what id does. If we double this parameter, what happens to the total number of model parameters?**"
      ],
      "metadata": {
        "id": "6tjo4Tyu_zAw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**YOUR ANSWER**"
      ],
      "metadata": {
        "id": "u82DFB4cAhjz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What does the temperature paramter do?** See the generate method in [model.py](https://github.com/karpathy/minGPT/blob/37baab71b9abea1b76ab957409a1cc2fbfba8a26/mingpt/model.py#L283).\n",
        "\n",
        "Try setting temperature to different values. What do you observe about the output?"
      ],
      "metadata": {
        "id": "r8adX_STRbjJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**YOUR ANSWER**"
      ],
      "metadata": {
        "id": "l_4ffALfRuET"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What does [line 148](https://github.com/karpathy/minGPT/blob/37baab71b9abea1b76ab957409a1cc2fbfba8a26/mingpt/model.py#L148) in model.py do? How does this relate to the transformer model?**  "
      ],
      "metadata": {
        "id": "IxqGbnN0oXib"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**YOUR ANSWER**"
      ],
      "metadata": {
        "id": "gbcrKfRSogw5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word Model\n",
        "Now, let's fit a word model instead of a character model.\n",
        "\n",
        "Given a string like:\n",
        "\n",
        "> The cow     jumped over the moon. The moon is full tonight!\n",
        "\n",
        "The `WordDataset` class below should create tokens for each space-delimited string:\n",
        "\n",
        "> ['The', 'cow', 'jumped', 'over', 'the', 'moon', '.', 'The', 'moon', 'is', 'full', 'tonight', '!']\n",
        "\n",
        "Note that multiple space characters are treated as one (Hint: `re` may help here.)\n",
        "\n",
        "Using `CharDataset::parse_data` function above as an example, complete the `parse_data` function below to set the `stoi`, `itos`, `vocab_size`, and `data` attributes of the `WordDataset` class."
      ],
      "metadata": {
        "id": "ijvSHpxC9dtg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "class WordDataset(CharDataset):\n",
        "  def parse_data(self, data):\n",
        "    \"\"\"\n",
        "    data.....A single string representing many sentences.\n",
        "    \"\"\"\n",
        "    ### YOUR CODE HERE\n",
        "    pass\n",
        "\n",
        "word_config = configure_model(max_iters=200, block_size=4)\n",
        "word_data = WordDataset(word_config.data, 'The cow jumped over the moon. The moon is full tonight!')\n",
        "word_data.data"
      ],
      "metadata": {
        "id": "u7Lyti4f7jdH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we can now reuse the training code to fit the word language model.\n",
        "def sample_from_word_model(context, model, trainer, train_dataset, maxlen=500, temperature=1.):\n",
        "    x = torch.tensor([train_dataset.stoi[s] for s in context], dtype=torch.long)[None,...].to(trainer.device)\n",
        "    y = model.generate(x, maxlen, temperature=temperature, do_sample=True, top_k=10)[0]\n",
        "    return ' '.join([train_dataset.itos[int(i)] for i in y])\n",
        "\n",
        "word_model, word_trainer = train_model(word_config, word_data, sample_from_word_model)"
      ],
      "metadata": {
        "id": "ftCK_2L8-uHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_from_word_model([\"The\"], word_model, word_trainer, word_data, maxlen=50, temperature=1.)"
      ],
      "metadata": {
        "id": "fZweO3GBIKsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Wikipedia\n",
        "\n",
        "With our word model, let's now fit a language model on the Wikipedia page for [New Orleans](https://en.wikipedia.org/wiki/New_Orleans)\n",
        "\n",
        "First, we'll install a library to help us fetch the plain text of a wikipedia page."
      ],
      "metadata": {
        "id": "-lYiFOD3MVhy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wikipedia"
      ],
      "metadata": {
        "id": "RnhwHz7-McWP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wikipedia\n",
        "wikipedia.set_lang('en')\n",
        "page = wikipedia.page('New Orleans')\n",
        "print(page.content[:100])"
      ],
      "metadata": {
        "id": "A3pclbQqL2zb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create new variables `word_config`, `word_data`, `word_model`, `word_trainer` that are analogous to the ones used previously. These should fit a model to the `page` text defined in the previous cell.**"
      ],
      "metadata": {
        "id": "6OB56CJpkKKa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE\n"
      ],
      "metadata": {
        "id": "jVcV90oKMjaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_from_word_model([\"A\", \"local\", \"variant\", \"for\", \"hip\", \"hop\", \"is\"], word_model, word_trainer, word_data, maxlen=200, temperature=1.)"
      ],
      "metadata": {
        "id": "cg1v3nK2QKi-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Investigate different model settings (`block_size, max_iters, learning_rate, n_embd, n_layer`).\n",
        "\n",
        "**What effect do you notice from trying different values? Which setting appears to generate the best generated text?**\n"
      ],
      "metadata": {
        "id": "LjKzZ1gHglDH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**YOUR ANSWER**\n",
        "\n"
      ],
      "metadata": {
        "id": "mHxKPzVag3E4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suppose you wanted to take the word model trained on the New Orleans Wikipedia page and use supervised fine-tuning to create a chatbot that answers questions about New Orleans.\n",
        "\n",
        "**What type of additional training data would you need to do this?**\n",
        "\n",
        "Provide example data below."
      ],
      "metadata": {
        "id": "jkacGUNjVwhh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**YOUR ANSWER**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QPCc4ZlngSTG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**If this new data contains words that don't appear in the New Orleans wikipedia page, what will happen? How can you fix this?**"
      ],
      "metadata": {
        "id": "qEM97S3xgTjd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**YOUR ANSWER**"
      ],
      "metadata": {
        "id": "ZcO_yJkxgdzf"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}